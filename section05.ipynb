{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e62f530-4e55-4421-b79f-ae0a4e1d5431",
   "metadata": {},
   "source": [
    "# https://nlp100.github.io/ja/ch05.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88df4d-3cbd-4726-9108-36baccce468a",
   "metadata": {},
   "source": [
    "## 第5章: 係り受け解析  \n",
    "日本語Wikipediaの「人工知能」に関する記事からテキスト部分を抜き出したファイルがai.ja.zipに収録されている． この文章をCaboChaやKNP等のツールを利用して係り受け解析を行い，その結果をai.ja.txt.parsedというファイルに保存せよ．このファイルを読み込み，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4192c-3bf3-46e9-be8b-8c5e796a5483",
   "metadata": {},
   "source": [
    "ステップ1: 依存関係のインストール  \n",
    "sudo apt update  \n",
    "sudo apt install -y build-essential curl file git  \n",
    "sudo apt install -y mecab libmecab-dev mecab-ipadic-utf8  \n",
    "sudo apt install -y zlib1g-dev libcurl4-openssl-dev libxml2-dev  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7485f77-2c8e-4562-a747-446735d2ce80",
   "metadata": {},
   "source": [
    "ステップ2: CRF++のインストール  \n",
    "git clone https://github.com/taku910/crfpp.git  \n",
    "cd crfpp  \n",
    "./configure  \n",
    "make  \n",
    "sudo make install  \n",
    "cd ..  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb48f19-311f-442a-b088-e34e5833790e",
   "metadata": {},
   "source": [
    "ステップ3: CaboChaのダウンロードとビルド  \n",
    "git clone https://github.com/taku910/cabocha.git  \n",
    "cd cabocha  \n",
    "sudo apt-get install autoconf automake libtool  \n",
    "autoreconf -i  \n",
    "./configure --with-mecab-config=`which mecab-config` --with-charset=utf8  \n",
    "make  \n",
    "sudo make install  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4848d-3ceb-4fec-a7e6-f9de6aa1f709",
   "metadata": {},
   "source": [
    "swig_import_helper`について、以下の部分を修正\n",
    "```\n",
    "def swig_import_helper():\n",
    "    import os  \n",
    "    import importlib.util  \n",
    "    spec = importlib.util.find_spec('_CaboCha', [os.path.dirname(__file__)])  \n",
    "    if spec is None:  \n",
    "        raise ImportError('_CaboCha module not found')  \n",
    "    _mod = importlib.util.module_from_spec(spec)  \n",
    "    spec.loader.exec_module(_mod)  \n",
    "    return _mod  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105fa60d-0b1c-48e6-857a-165bb8f35941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iconv_open is not supported\n"
     ]
    }
   ],
   "source": [
    "import CaboCha\n",
    "CBC = CaboCha.Parser()\n",
    "divtext = []\n",
    "with open(\"./datafiles/ai.ja.txt\", \"r\") as f, open(\"./datafiles/ai.ja.txt.parsed\", \"w\") as f2:\n",
    "    lines = f.readlines()\n",
    "    for text in lines:\n",
    "        if \"。\" in text:\n",
    "            temp = text.split(\"。\")\n",
    "            temp = [x + \"。\" for x in temp if x != '']\n",
    "            divtext.extend(temp)\n",
    "    for text in divtext:\n",
    "        tree = CBC.parse(text)\n",
    "        f2.write(tree.toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "532eac8c-92cb-4886-8290-905939f89597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\n",
      "\n",
      "人工知能（じんこうちのう、、AI〈エーアイ〉）とは、「『計算（）』という概念と『コンピュータ（）』という道具を用いて『知能』を研究する計算機科学（）の一分野」を指す語。「言語の理解や推論、問題解決などの知的行動を人間に代わってコンピューターに行わせる技術」、または、「計算機（コンピュータ）による知的な情報処理システムの設計や実現に関する研究分野」ともされる。\n",
      "\n",
      "『日本大百科全書(ニッポニカ)』の解説で、情報工学者・通信工学者の佐藤理史は次のように述べている。\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./datafiles/ai.ja.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e9d72b-21d4-4843-8584-52331aa608d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 14D 1/1 -1.776924\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
      "* 1 14D 2/3 -1.776924\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "じん\t名詞,一般,*,*,*,*,じん,ジン,ジン\n",
      "こうち\t名詞,一般,*,*,*,*,こうち,コウチ,コーチ\n",
      "のう\t助詞,終助詞,*,*,*,*,のう,ノウ,ノー\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "* 2 3D 0/0 0.592011\n",
      "AI\t名詞,一般,*,*,*,*,*\n",
      "* 3 14D 1/5 -1.776924\n",
      "〈\t記号,括弧開,*,*,*,*,〈,〈,〈\n",
      "エーアイ\t名詞,固有名詞,一般,*,*,*,*\n",
      "〉\t記号,括弧閉,*,*,*,*,〉,〉,〉\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n"
     ]
    }
   ],
   "source": [
    "!head -20 ./datafiles/ai.ja.txt.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03025a1-eac5-47b2-952e-d2ddf2c7f15c",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "095078d3-41c4-4816-bcfd-0d89dd82e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冒頭の文の形態素列:\n",
      "Morph(surface='人工', base='人工', pos='名詞', pos1='一般')\n",
      "Morph(surface='知能', base='知能', pos='名詞', pos1='一般')\n",
      "Morph(surface='（', base='（', pos='記号', pos1='括弧開')\n",
      "Morph(surface='じん', base='じん', pos='名詞', pos1='一般')\n",
      "Morph(surface='こうち', base='こうち', pos='名詞', pos1='一般')\n",
      "Morph(surface='のう', base='のう', pos='助詞', pos1='終助詞')\n",
      "Morph(surface='、', base='、', pos='記号', pos1='読点')\n",
      "Morph(surface='、', base='、', pos='記号', pos1='読点')\n",
      "Morph(surface='AI', base='*', pos='名詞', pos1='一般')\n",
      "Morph(surface='〈', base='〈', pos='記号', pos1='括弧開')\n",
      "Morph(surface='エーアイ', base='*', pos='名詞', pos1='固有名詞')\n",
      "Morph(surface='〉', base='〉', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='）', base='）', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='と', base='と', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='は', base='は', pos='助詞', pos1='係助詞')\n",
      "Morph(surface='、', base='、', pos='記号', pos1='読点')\n",
      "Morph(surface='「', base='「', pos='記号', pos1='括弧開')\n",
      "Morph(surface='『', base='『', pos='記号', pos1='括弧開')\n",
      "Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続')\n",
      "Morph(surface='（', base='（', pos='記号', pos1='括弧開')\n",
      "Morph(surface='）', base='）', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='』', base='』', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='という', base='という', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='概念', base='概念', pos='名詞', pos1='一般')\n",
      "Morph(surface='と', base='と', pos='助詞', pos1='並立助詞')\n",
      "Morph(surface='『', base='『', pos='記号', pos1='括弧開')\n",
      "Morph(surface='コンピュータ', base='コンピュータ', pos='名詞', pos1='一般')\n",
      "Morph(surface='（', base='（', pos='記号', pos1='括弧開')\n",
      "Morph(surface='）', base='）', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='』', base='』', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='という', base='という', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='道具', base='道具', pos='名詞', pos1='一般')\n",
      "Morph(surface='を', base='を', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='用い', base='用いる', pos='動詞', pos1='自立')\n",
      "Morph(surface='て', base='て', pos='助詞', pos1='接続助詞')\n",
      "Morph(surface='『', base='『', pos='記号', pos1='括弧開')\n",
      "Morph(surface='知能', base='知能', pos='名詞', pos1='一般')\n",
      "Morph(surface='』', base='』', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='を', base='を', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='研究', base='研究', pos='名詞', pos1='サ変接続')\n",
      "Morph(surface='する', base='する', pos='動詞', pos1='自立')\n",
      "Morph(surface='計算', base='計算', pos='名詞', pos1='サ変接続')\n",
      "Morph(surface='機', base='機', pos='名詞', pos1='接尾')\n",
      "Morph(surface='科学', base='科学', pos='名詞', pos1='一般')\n",
      "Morph(surface='（', base='（', pos='記号', pos1='括弧開')\n",
      "Morph(surface='）', base='）', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='の', base='の', pos='助詞', pos1='連体化')\n",
      "Morph(surface='一', base='一', pos='名詞', pos1='数')\n",
      "Morph(surface='分野', base='分野', pos='名詞', pos1='一般')\n",
      "Morph(surface='」', base='」', pos='記号', pos1='括弧閉')\n",
      "Morph(surface='を', base='を', pos='助詞', pos1='格助詞')\n",
      "Morph(surface='指す', base='指す', pos='動詞', pos1='自立')\n",
      "Morph(surface='語', base='語', pos='名詞', pos1='一般')\n",
      "Morph(surface='。', base='。', pos='記号', pos1='句点')\n"
     ]
    }
   ],
   "source": [
    "class Morph:\n",
    "    def __init__(self, pos):\n",
    "        \"\"\"\n",
    "        形態素を初期化します。\n",
    "        :param pos: 形態素情報が格納されたリスト\n",
    "        \"\"\"\n",
    "        self.surface = pos[0]  # 表層形\n",
    "        self.base = pos[7]     # 基本形\n",
    "        self.pos = pos[1]      # 品詞\n",
    "        self.pos1 = pos[2]     # 品詞細分類1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Morph(surface='{self.surface}', base='{self.base}', pos='{self.pos}', pos1='{self.pos1}')\"\n",
    "\n",
    "    @classmethod # Morphクラスから直接メソッドを呼び出すために指定\n",
    "    def parse_from_file(cls, file_path):\n",
    "        \"\"\"\n",
    "        形態素解析結果を読み込み、文ごとの Morph オブジェクトのリストを生成します。\n",
    "        :param file_path: ファイルのパス\n",
    "        :return: 文ごとの Morph オブジェクトのリスト\n",
    "        \"\"\"\n",
    "        morph_list = []\n",
    "        sentence = []\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()  # 空白や改行を除去\n",
    "                if line == \"EOS\":  # 文の区切り\n",
    "                    if sentence:\n",
    "                        morph_list.append(sentence)\n",
    "                        sentence = []\n",
    "                elif not line.startswith(\"*\"):  # 係り受け情報をスキップ\n",
    "                    cols = line.split(\"\\t\")\n",
    "                    if len(cols) > 1:\n",
    "                        features = cols[1].split(\",\")\n",
    "                        pos = [cols[0]] + features\n",
    "                        sentence.append(cls(pos))  # クラスメソッドを使ってインスタンスを生成\n",
    "\n",
    "        return morph_list\n",
    "\n",
    "\n",
    "# ファイルパスを指定して実行\n",
    "file_path = \"./datafiles/ai.ja.txt.parsed\"\n",
    "morph_list = Morph.parse_from_file(file_path)\n",
    "\n",
    "# 冒頭の文の形態素列を表示\n",
    "if morph_list:\n",
    "    print(\"冒頭の文の形態素列:\")\n",
    "    for morph in morph_list[0]:\n",
    "        print(morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f23d-ec9d-4720-ab70-e08e32541d66",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）  \n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb3a09b2-0830-4dc0-a80c-a46daafca1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冒頭の文の文節と係り先:\n",
      "文節 0: 人工知能 -> なし\n",
      "文節 1: じんこうちのう -> なし\n",
      "文節 2: AI -> エーアイとは\n",
      "文節 3: エーアイとは -> なし\n",
      "文節 4: 計算という -> コンピュータという\n",
      "文節 5: 概念と -> コンピュータという\n",
      "文節 6: コンピュータという -> 道具を\n",
      "文節 7: 道具を -> 用いて\n",
      "文節 8: 用いて -> 研究する\n",
      "文節 9: 知能を -> 研究する\n",
      "文節 10: 研究する -> 計算機科学の\n",
      "文節 11: 計算機科学の -> 一分野を\n",
      "文節 12: 一分野を -> 指す\n",
      "文節 13: 指す -> なし\n"
     ]
    }
   ],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        \"\"\"\n",
    "        文節を表すクラス\n",
    "        :param morphs: Morphオブジェクトのリスト\n",
    "        :param dst: 係り先文節インデックス番号\n",
    "        \"\"\"\n",
    "        self.morphs = morphs  # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst        # 係り先文節インデックス番号\n",
    "        self.srcs = []        # 係り元文節インデックス番号のリスト\n",
    "\n",
    "    def __repr__(self):\n",
    "        morph_text = ''.join([morph.surface for morph in self.morphs if morph.pos != \"記号\"])\n",
    "        return f\"Chunk(text='{morph_text}', dst={self.dst}, srcs={self.srcs})\"\n",
    "\n",
    "\n",
    "def parse_chunks(file_path):\n",
    "    \"\"\"\n",
    "    係り受け解析結果を読み込み、文ごとの Chunk オブジェクトのリストを生成する。\n",
    "    :param file_path: ファイルのパス\n",
    "    :return: 文ごとの Chunk オブジェクトのリスト\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    morphs = []\n",
    "    dst = -1\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"EOS\":\n",
    "                if chunks:\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        # dstが有効な範囲内であることを確認\n",
    "                        if 0 <= chunk.dst < len(chunks):\n",
    "                            chunks[chunk.dst].srcs.append(i)\n",
    "                    sentences.append(chunks)\n",
    "                chunks = []\n",
    "            elif line.startswith(\"*\"):\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs, dst))\n",
    "                    morphs = []\n",
    "                _, idx, dst, *_ = line.split()\n",
    "                dst = int(dst.rstrip(\"D\"))\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                if len(cols) > 1:\n",
    "                    features = cols[1].split(\",\")\n",
    "                    pos = [cols[0]] + features\n",
    "                    morphs.append(Morph(pos))\n",
    "        if morphs:\n",
    "            chunks.append(Chunk(morphs, dst))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ファイルパスを指定して実行\n",
    "file_path = \"./datafiles/ai.ja.txt.parsed\"\n",
    "chunk_list = parse_chunks(file_path)\n",
    "\n",
    "# 冒頭の文の文節と係り先を表示\n",
    "if chunk_list:\n",
    "    print(\"冒頭の文の文節と係り先:\")\n",
    "    for i, chunk in enumerate(chunk_list[0]):\n",
    "        # 文節の文字列を取得\n",
    "        chunk_text = ''.join([m.surface for m in chunk.morphs if m.pos != \"記号\"])\n",
    "        \n",
    "        # 係り先の文字列を取得（範囲チェック付き）\n",
    "        if 0 <= chunk.dst < len(chunk_list[0]):\n",
    "            dst_text = ''.join([m.surface for m in chunk_list[0][chunk.dst].morphs if m.pos != \"記号\"])\n",
    "        else:\n",
    "            dst_text = \"なし\"\n",
    "        \n",
    "        print(f\"文節 {i}: {chunk_text} -> {dst_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c717966-ccb6-47fd-8146-468fc5ecbad8",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示  \n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cac7c824-6198-452c-a140-a1960d5baeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冒頭3文の文節と係り先:\n",
      "\n",
      "【文 1】\n",
      "文節 0: 人工知能\tなし\n",
      "文節 1: じんこうちのう\tなし\n",
      "文節 2: AI\tエーアイとは\n",
      "文節 3: エーアイとは\tなし\n",
      "文節 4: 計算という\tコンピュータという\n",
      "文節 5: 概念と\tコンピュータという\n",
      "文節 6: コンピュータという\t道具を\n",
      "文節 7: 道具を\t用いて\n",
      "文節 8: 用いて\t研究する\n",
      "文節 9: 知能を\t研究する\n",
      "文節 10: 研究する\t計算機科学の\n",
      "文節 11: 計算機科学の\t一分野を\n",
      "文節 12: 一分野を\t指す\n",
      "文節 13: 指す\tなし\n",
      "\n",
      "【文 2】\n",
      "文節 0: 語\tなし\n",
      "文節 1: 言語の\t理解や\n",
      "文節 2: 理解や\t理解や\n",
      "文節 3: 推論\t推論\n",
      "文節 4: 問題解決などの\t問題解決などの\n",
      "文節 5: 知的行動を\t人間に\n",
      "文節 6: 人間に\t人間に\n",
      "文節 7: 代わって\tコンピューターに\n",
      "文節 8: コンピューターに\tコンピューターに\n",
      "文節 9: 行わせる\t行わせる\n",
      "文節 10: 技術または\t実現に関する\n",
      "文節 11: 計算機コンピュータによる\t知的な\n",
      "文節 12: 知的な\t知的な\n",
      "文節 13: 情報処理システムの\t設計や\n",
      "文節 14: 設計や\t設計や\n",
      "文節 15: 実現に関する\t実現に関する\n",
      "文節 16: 研究分野とも\t研究分野とも\n",
      "\n",
      "【文 3】\n",
      "文節 0: される\tなし\n"
     ]
    }
   ],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        \"\"\"\n",
    "        文節を表すクラス\n",
    "        :param morphs: Morphオブジェクトのリスト\n",
    "        :param dst: 係り先文節インデックス番号\n",
    "        \"\"\"\n",
    "        self.morphs = morphs  # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst        # 係り先文節インデックス番号\n",
    "        self.srcs = []        # 係り元文節インデックス番号のリスト\n",
    "\n",
    "    def __repr__(self):\n",
    "        morph_text = ''.join([morph.surface for morph in self.morphs if morph.pos != \"記号\"])\n",
    "        return f\"Chunk(text='{morph_text}', dst={self.dst}, srcs={self.srcs})\"\n",
    "\n",
    "\n",
    "def parse_chunks(file_path):\n",
    "    \"\"\"\n",
    "    係り受け解析結果を読み込み、文ごとの Chunk オブジェクトのリストを生成する。\n",
    "    :param file_path: ファイルのパス\n",
    "    :return: 文ごとの Chunk オブジェクトのリスト\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    morphs = []\n",
    "    dst = -1\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"EOS\":\n",
    "                if chunks:\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        # dstが有効な範囲内であることを確認\n",
    "                        if 0 <= chunk.dst < len(chunks):\n",
    "                            chunks[chunk.dst].srcs.append(i)\n",
    "                    sentences.append(chunks)\n",
    "                chunks = []\n",
    "            elif line.startswith(\"*\"):\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs, dst))\n",
    "                    morphs = []\n",
    "                _, idx, dst, *_ = line.split()\n",
    "                dst = int(dst.rstrip(\"D\"))\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                if len(cols) > 1:\n",
    "                    features = cols[1].split(\",\")\n",
    "                    pos = [cols[0]] + features\n",
    "                    morphs.append(Morph(pos))\n",
    "        if morphs:\n",
    "            chunks.append(Chunk(morphs, dst))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ファイルパスを指定して実行\n",
    "file_path = \"./datafiles/ai.ja.txt.parsed\"\n",
    "chunk_list = parse_chunks(file_path)\n",
    "\n",
    "# 冒頭の文の文節と係り先を表示\n",
    "if chunk_list:\n",
    "    print(\"冒頭3文の文節と係り先:\")\n",
    "    for sentence_idx, sentence in enumerate(chunk_list[:3]):  # 冒頭3文\n",
    "        print(f\"\\n【文 {sentence_idx + 1}】\")\n",
    "        for i, chunk in enumerate(sentence):\n",
    "            # 文節の文字列を取得\n",
    "            chunk_text = ''.join([m.surface for m in chunk.morphs if m.pos != \"記号\"])\n",
    "            \n",
    "            # 係り先の文字列を取得（範囲チェック付き）\n",
    "            if 0 <= chunk.dst < len(sentence):\n",
    "                dst_text = ''.join([m.surface for m in sentence[chunk.dst].morphs if m.pos != \"記号\"])\n",
    "            else:\n",
    "                dst_text = \"なし\"\n",
    "            \n",
    "            print(f\"文節 {i}: {chunk_text}\\t{dst_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13add8f0-7a5e-4bf3-af91-eedb3fcf4dbb",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出  \n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ce9a6b-5109-4a11-89d9-992d77240c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Morphクラスの定義\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        \"\"\"\n",
    "        形態素を表すクラス\n",
    "        :param surface: 表層形\n",
    "        :param base: 基本形\n",
    "        :param pos: 品詞\n",
    "        :param pos1: 品詞細分類1\n",
    "        \"\"\"\n",
    "        self.surface = surface  # 表層形\n",
    "        self.base = base        # 基本形\n",
    "        self.pos = pos          # 品詞\n",
    "        self.pos1 = pos1        # 品詞細分類1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Morph(surface='{self.surface}', base='{self.base}', pos='{self.pos}', pos1='{self.pos1}')\"\n",
    "\n",
    "\n",
    "# Chunkクラスの定義\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        \"\"\"\n",
    "        文節を表すクラス\n",
    "        :param morphs: Morphオブジェクトのリスト\n",
    "        :param dst: 係り先文節インデックス番号\n",
    "        \"\"\"\n",
    "        self.morphs = morphs  # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst        # 係り先文節インデックス番号\n",
    "        self.srcs = []        # 係り元文節インデックス番号のリスト\n",
    "\n",
    "    def __repr__(self):\n",
    "        morph_text = ''.join([morph.surface for morph in self.morphs if morph.pos != \"記号\"])\n",
    "        return f\"Chunk(text='{morph_text}', dst={self.dst}, srcs={self.srcs})\"\n",
    "\n",
    "\n",
    "def parse_chunks(file_path):\n",
    "    \"\"\"\n",
    "    係り受け解析結果を読み込み、文ごとの Chunk オブジェクトのリストを生成する。\n",
    "    :param file_path: ファイルのパス\n",
    "    :return: 文ごとの Chunk オブジェクトのリスト\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    morphs = []\n",
    "    dst = -1\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"EOS\":\n",
    "                if chunks:\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        # dstが有効な範囲内であることを確認\n",
    "                        if 0 <= chunk.dst < len(chunks):\n",
    "                            chunks[chunk.dst].srcs.append(i)\n",
    "                    sentences.append(chunks)\n",
    "                chunks = []\n",
    "                morphs = []\n",
    "                dst = -1\n",
    "            elif line.startswith(\"*\"):\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs, dst))\n",
    "                    morphs = []\n",
    "                parts = line.split()\n",
    "                dst = int(parts[2].rstrip(\"D\"))\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                if len(cols) > 1:\n",
    "                    surface = cols[0]\n",
    "                    feature = cols[1].split(\",\")\n",
    "                    base = feature[6]\n",
    "                    pos = feature[0]\n",
    "                    pos1 = feature[1]\n",
    "                    morphs.append(Morph(surface, base, pos, pos1))\n",
    "        # 最後の文の処理\n",
    "        if morphs:\n",
    "            chunks.append(Chunk(morphs, dst))\n",
    "        if chunks:\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if 0 <= chunk.dst < len(chunks):\n",
    "                    chunks[chunk.dst].srcs.append(i)\n",
    "            sentences.append(chunks)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ファイルパスを指定して解析を実行\n",
    "file_path = \"./datafiles/ai.ja.txt.parsed\"\n",
    "chunk_list = parse_chunks(file_path)\n",
    "\n",
    "# 名詞を含む文節が動詞を含む文節に係る場合を抽出してタブ区切りで出力\n",
    "with open(\"./datafiles/noun_to_verb_dependencies.txt\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for sentence in chunk_list:\n",
    "        for chunk in sentence:\n",
    "            # 文節に名詞が含まれているかチェック\n",
    "            if any(morph.pos == \"名詞\" for morph in chunk.morphs):\n",
    "                # 係り先が存在するか確認\n",
    "                if 0 <= chunk.dst < len(sentence):\n",
    "                    dst_chunk = sentence[chunk.dst]\n",
    "                    # 係り先の文節に動詞が含まれているかチェック\n",
    "                    if any(morph.pos == \"動詞\" for morph in dst_chunk.morphs):\n",
    "                        # 記号を除いた文節の文字列を取得\n",
    "                        src_text = ''.join([morph.surface for morph in chunk.morphs if morph.pos != \"記号\"])\n",
    "                        dst_text = ''.join([morph.surface for morph in dst_chunk.morphs if morph.pos != \"記号\"])\n",
    "                        # タブ区切りで出力\n",
    "                        output_file.write(f\"{src_text}\\t{dst_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857e9fa-6282-47ea-8e65-fb5c125c2b0c",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．\n",
    "\n",
    "* 日本語の文字が文字化けしている。解消方法不明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4188b76-521e-4193-b164-5d381e5b6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "グラフが以下に保存されました: dependency_tree.png\n"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "# Morphクラスの定義が必要です。以下は仮の定義です。\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Morph(surface='{self.surface}', base='{self.base}', pos='{self.pos}', pos1='{self.pos1}')\"\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        \"\"\"\n",
    "        文節を表すクラス\n",
    "        :param morphs: Morphオブジェクトのリスト\n",
    "        :param dst: 係り先文節インデックス番号\n",
    "        \"\"\"\n",
    "        self.morphs = morphs  # 形態素（Morphオブジェクト）のリスト\n",
    "        self.dst = dst        # 係り先文節インデックス番号\n",
    "        self.srcs = []        # 係り元文節インデックス番号のリスト\n",
    "\n",
    "    def __repr__(self):\n",
    "        morph_text = ''.join([morph.surface for morph in self.morphs if morph.pos != \"記号\"])\n",
    "        return f\"Chunk(text='{morph_text}', dst={self.dst}, srcs={self.srcs})\"\n",
    "\n",
    "\n",
    "def parse_chunks(file_path):\n",
    "    \"\"\"\n",
    "    係り受け解析結果を読み込み、文ごとの Chunk オブジェクトのリストを生成する。\n",
    "    :param file_path: ファイルのパス\n",
    "    :return: 文ごとの Chunk オブジェクトのリスト\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    morphs = []\n",
    "    dst = -1\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"EOS\":\n",
    "                if chunks:\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        # dstが有効な範囲内であることを確認\n",
    "                        if 0 <= chunk.dst < len(chunks):\n",
    "                            chunks[chunk.dst].srcs.append(i)\n",
    "                    sentences.append(chunks)\n",
    "                chunks = []\n",
    "            elif line.startswith(\"*\"):\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs, dst))\n",
    "                    morphs = []\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    _, idx, dst_str = parts[:3]\n",
    "                    dst = int(dst_str.rstrip(\"D\"))\n",
    "                else:\n",
    "                    dst = -1\n",
    "            else:\n",
    "                cols = line.split(\"\\t\")\n",
    "                if len(cols) > 1:\n",
    "                    surface = cols[0]\n",
    "                    features = cols[1].split(\",\")\n",
    "                    if len(features) >= 7:\n",
    "                        base = features[6]\n",
    "                    else:\n",
    "                        base = \"*\"\n",
    "                    pos = features[0]\n",
    "                    pos1 = features[1]\n",
    "                    morphs.append(Morph(surface, base, pos, pos1))\n",
    "        if morphs:\n",
    "            chunks.append(Chunk(morphs, dst))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def visualize_dependency_tree(sentence, file_name=\"dependency_tree\"):\n",
    "    \"\"\"\n",
    "    1文の係り受け木をGraphvizで可視化する。\n",
    "    :param sentence: 文（Chunkオブジェクトのリスト）\n",
    "    :param file_name: 出力ファイル名\n",
    "    \"\"\"\n",
    "    dot = graphviz.Digraph(format=\"png\")\n",
    "    # DPIの設定\n",
    "    dot.attr(dpi=\"300\")\n",
    "    # ノードのフォントを設定\n",
    "    dot.attr('node', fontname=\"Noto Sans CJK JP\")  \n",
    "\n",
    "    for i, chunk in enumerate(sentence):\n",
    "        # ノードのラベルは文節のテキスト\n",
    "        chunk_text = ''.join([m.surface for m in chunk.morphs if m.pos != \"記号\"])\n",
    "        dot.node(str(i), chunk_text)\n",
    "\n",
    "        # 係り先がある場合、エッジを追加\n",
    "        if 0 <= chunk.dst < len(sentence):\n",
    "            dot.edge(str(i), str(chunk.dst))\n",
    "\n",
    "    # グラフの出力と表示\n",
    "    output_path = dot.render(file_name)\n",
    "    print(f\"グラフが以下に保存されました: {output_path}\")\n",
    "\n",
    "# 使用例\n",
    "# 最初の文の係り受け木を可視化\n",
    "file_path = \"./datafiles/ai.ja.txt.parsed\"\n",
    "chunk_lists = parse_chunks(file_path)\n",
    "if chunk_lists:\n",
    "    first_sentence = chunk_lists[0]\n",
    "    visualize_dependency_tree(first_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf3aca-1aeb-4273-b05c-286d81578528",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "述語に係る助詞を格とする\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d476f38-606d-4be3-a101-066111ca917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力を ./datafiles/predicate_cases.txt に保存しました。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface  # 表層形\n",
    "        self.base = base        # 基本形\n",
    "        self.pos = pos          # 品詞\n",
    "        self.pos1 = pos1        # 品詞細分類1\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs      # Morphのリスト\n",
    "        self.dst = dst            # 係り先のチャンク番号\n",
    "        self.srcs = []            # 係り元のチャンク番号\n",
    "\n",
    "def parse_cabocha(filename):\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        chunks = {}\n",
    "        morphs = []\n",
    "        dst = -1\n",
    "        idx = -1  # 初期化\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('*'):\n",
    "                if morphs:\n",
    "                    chunks[idx] = Chunk(morphs, dst)\n",
    "                    morphs = []\n",
    "                parts = line.split()\n",
    "                idx = int(parts[1])\n",
    "                # 修正: '-' が付く場合と付かない場合の両方に対応\n",
    "                match = re.search(r'(-?\\d+)D', parts[2])\n",
    "                if match:\n",
    "                    dst = int(match.group(1))\n",
    "                else:\n",
    "                    # マッチしない場合のエラーハンドリング\n",
    "                    print(f\"Warning: 予期せぬ形式の依存先: {parts[2]}\")\n",
    "                    dst = -1\n",
    "                chunks[idx] = Chunk([], dst)\n",
    "            elif line == 'EOS':\n",
    "                if chunks:\n",
    "                    # 係り元を集める\n",
    "                    for idx_key, chunk in chunks.items():\n",
    "                        if chunk.dst != -1:\n",
    "                            if chunk.dst in chunks:\n",
    "                                chunks[chunk.dst].srcs.append(idx_key)\n",
    "                            else:\n",
    "                                print(f\"Warning: 存在しないチャンク番号 {chunk.dst} への依存\")\n",
    "                    sentences.append(chunks)\n",
    "                    chunks = {}\n",
    "            else:\n",
    "                if '\\t' in line:\n",
    "                    surface, feature = line.split('\\t')\n",
    "                    features = feature.split(',')\n",
    "                    if len(features) >= 7:\n",
    "                        morph = Morph(surface, features[6], features[0], features[1])\n",
    "                        chunks[idx].morphs.append(morph)\n",
    "    return sentences\n",
    "\n",
    "def extract_predicate_case(sentences):\n",
    "    results = []\n",
    "    for chunks in sentences:\n",
    "        predicate = None\n",
    "        cases = []\n",
    "        predicate_chunk = None\n",
    "        # 述語を探す（最左の動詞）\n",
    "        for idx in sorted(chunks.keys()):\n",
    "            chunk = chunks[idx]\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    predicate = morph.base\n",
    "                    predicate_chunk = chunk\n",
    "                    break\n",
    "            if predicate:\n",
    "                break\n",
    "        if predicate and predicate_chunk:\n",
    "            # 述語に係る助詞を探す\n",
    "            for src in predicate_chunk.srcs:\n",
    "                src_chunk = chunks[src]\n",
    "                for morph in src_chunk.morphs:\n",
    "                    if morph.pos == '助詞':\n",
    "                        cases.append(morph.surface)\n",
    "                        break  # 一つの助詞で十分\n",
    "            if cases:\n",
    "                # 辞書順に並べる\n",
    "                cases = sorted(cases)\n",
    "                # 重複を除去\n",
    "                cases = list(dict.fromkeys(cases))\n",
    "                results.append(f\"{predicate}\\t{' '.join(cases)}\")\n",
    "            else:\n",
    "                # 助詞がない場合の処理\n",
    "                results.append(f\"{predicate}\\t\")\n",
    "    return results\n",
    "\n",
    "# 入力ファイルと出力ファイルを指定\n",
    "input_filename = './datafiles/ai.ja.txt.parsed'  # 実際のパスに変更してください\n",
    "output_filename = './datafiles/predicate_cases.txt'  # 出力ファイル名\n",
    "\n",
    "# コーパスを解析\n",
    "sentences = parse_cabocha(input_filename)\n",
    "results = extract_predicate_case(sentences)\n",
    "\n",
    "# 出力ファイルに書き込む\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_f:\n",
    "    for line in results:\n",
    "        out_f.write(line + '\\n')\n",
    "\n",
    "print(f\"出力を {output_filename} に保存しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c46dbf-5a45-4852-b833-fe493677a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\n",
      "代わる\tに を\n",
      "述べる\tで の は\n",
      "する\tで を\n",
      "する\tを\n",
      "する\tで に により\n",
      "用いる\tを\n",
      "呼ぶ\tも\n",
      "する\tを\n",
      "知る\tとして も\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ./datafiles/predicate_cases.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65ac763b-96ec-4e02-9a78-9da6c44d9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sort ./datafiles/predicate_cases.txt | uniq -c | sort -nr > ./datafiles/predicate_case_frequencies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68e25897-41b2-4a40-97a3-a564c0bfe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15 する\tを\n",
      "      8 よる\tに\n",
      "      7 する\tが\n",
      "      5 する\tに を\n",
      "      5 する\tに\n",
      "      5 する\tが に\n",
      "      4 する\tは を\n",
      "      3 基づく\tに\n",
      "      3 向ける\tに\n",
      "      3 受ける\tを\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 ./datafiles/predicate_case_frequencies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e16be-6b3f-4278-9e8c-1235aaaf1c7e",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47ab6171-f2af-470b-ab73-e7251311c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力を ./datafiles/predicate_cases_with_chunks.txt に保存しました。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface  # 表層形\n",
    "        self.base = base        # 基本形\n",
    "        self.pos = pos          # 品詞\n",
    "        self.pos1 = pos1        # 品詞細分類1\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs      # Morphのリスト\n",
    "        self.dst = dst            # 係り先のチャンク番号\n",
    "        self.srcs = []            # 係り元のチャンク番号\n",
    "\n",
    "    def surface(self):\n",
    "        \"\"\"文節内の表層形を連結して返す\"\"\"\n",
    "        return ''.join(m.surface for m in self.morphs if m.pos != '記号')\n",
    "\n",
    "def parse_cabocha(filename):\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        chunks = {}\n",
    "        morphs = []\n",
    "        dst = -1\n",
    "        idx = -1  # 初期化\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('*'):\n",
    "                if morphs:\n",
    "                    chunks[idx] = Chunk(morphs, dst)\n",
    "                    morphs = []\n",
    "                parts = line.split()\n",
    "                idx = int(parts[1])\n",
    "                match = re.search(r'(-?\\d+)D', parts[2])\n",
    "                if match:\n",
    "                    dst = int(match.group(1))\n",
    "                else:\n",
    "                    print(f\"Warning: 予期せぬ形式の依存先: {parts[2]}\")\n",
    "                    dst = -1\n",
    "                chunks[idx] = Chunk([], dst)\n",
    "            elif line == 'EOS':\n",
    "                if chunks:\n",
    "                    for idx_key, chunk in chunks.items():\n",
    "                        if chunk.dst != -1:\n",
    "                            if chunk.dst in chunks:\n",
    "                                chunks[chunk.dst].srcs.append(idx_key)\n",
    "                            else:\n",
    "                                print(f\"Warning: 存在しないチャンク番号 {chunk.dst} への依存\")\n",
    "                    sentences.append(chunks)\n",
    "                    chunks = {}\n",
    "            else:\n",
    "                if '\\t' in line:\n",
    "                    surface, feature = line.split('\\t')\n",
    "                    features = feature.split(',')\n",
    "                    if len(features) >= 7:\n",
    "                        morph = Morph(surface, features[6], features[0], features[1])\n",
    "                        chunks[idx].morphs.append(morph)\n",
    "    return sentences\n",
    "\n",
    "def extract_predicate_case_with_chunks(sentences):\n",
    "    results = []\n",
    "    for chunks in sentences:\n",
    "        for idx in sorted(chunks.keys()):\n",
    "            chunk = chunks[idx]\n",
    "            predicate = None\n",
    "            cases = []\n",
    "            phrases = []\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    predicate = morph.base\n",
    "                    break\n",
    "            if predicate:\n",
    "                for src in chunk.srcs:\n",
    "                    src_chunk = chunks[src]\n",
    "                    for morph in src_chunk.morphs:\n",
    "                        if morph.pos == '助詞':\n",
    "                            cases.append(morph.surface)\n",
    "                            phrases.append(src_chunk.surface())\n",
    "                            break\n",
    "                if cases:\n",
    "                    cases, phrases = zip(*sorted(zip(cases, phrases)))  # 助詞でソート\n",
    "                    results.append(f\"{predicate}\\t{' '.join(cases)}\\t{' '.join(phrases)}\")\n",
    "    return results\n",
    "\n",
    "# 入力ファイルと出力ファイルを指定\n",
    "input_filename = './datafiles/ai.ja.txt.parsed'  # 実際のパスに変更してください\n",
    "output_filename = './datafiles/predicate_cases_with_chunks.txt'  # 出力ファイル名\n",
    "\n",
    "# コーパスを解析\n",
    "sentences = parse_cabocha(input_filename)\n",
    "results = extract_predicate_case_with_chunks(sentences)\n",
    "\n",
    "# 出力ファイルに書き込む\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_f:\n",
    "    for line in results:\n",
    "        out_f.write(line + '\\n')\n",
    "\n",
    "print(f\"出力を {output_filename} に保存しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48551ae-d050-4a4e-9683-ccc6daee1f88",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b18527c4-0655-43ac-b880-473b8857b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力を ./datafiles/sahen_cases.txt に保存しました。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface  # 表層形\n",
    "        self.base = base        # 基本形\n",
    "        self.pos = pos          # 品詞\n",
    "        self.pos1 = pos1        # 品詞細分類1\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs, dst):\n",
    "        self.morphs = morphs      # Morphのリスト\n",
    "        self.dst = dst            # 係り先のチャンク番号\n",
    "        self.srcs = []            # 係り元のチャンク番号\n",
    "\n",
    "    def surface(self):\n",
    "        \"\"\"文節内の表層形を連結して返す\"\"\"\n",
    "        return ''.join(m.surface for m in self.morphs if m.pos != '記号')\n",
    "\n",
    "    def has_sahen_wo(self):\n",
    "        \"\"\"サ変接続名詞+を（助詞）を含むか確認\"\"\"\n",
    "        for i in range(len(self.morphs) - 1):\n",
    "            if (self.morphs[i].pos1 == 'サ変接続' and\n",
    "                self.morphs[i + 1].surface == 'を' and\n",
    "                self.morphs[i + 1].pos == '助詞'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def sahen_wo_surface(self):\n",
    "        \"\"\"サ変接続名詞+をの表層形を返す\"\"\"\n",
    "        for i in range(len(self.morphs) - 1):\n",
    "            if (self.morphs[i].pos1 == 'サ変接続' and\n",
    "                self.morphs[i + 1].surface == 'を' and\n",
    "                self.morphs[i + 1].pos == '助詞'):\n",
    "                return self.morphs[i].surface + self.morphs[i + 1].surface\n",
    "        return ''\n",
    "\n",
    "def parse_cabocha(filename):\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        chunks = {}\n",
    "        morphs = []\n",
    "        dst = -1\n",
    "        idx = -1  # 初期化\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('*'):\n",
    "                if morphs:\n",
    "                    chunks[idx] = Chunk(morphs, dst)\n",
    "                    morphs = []\n",
    "                parts = line.split()\n",
    "                idx = int(parts[1])\n",
    "                match = re.search(r'(-?\\d+)D', parts[2])\n",
    "                if match:\n",
    "                    dst = int(match.group(1))\n",
    "                else:\n",
    "                    print(f\"Warning: 予期せぬ形式の依存先: {parts[2]}\")\n",
    "                    dst = -1\n",
    "                chunks[idx] = Chunk([], dst)\n",
    "            elif line == 'EOS':\n",
    "                if chunks:\n",
    "                    for idx_key, chunk in chunks.items():\n",
    "                        if chunk.dst != -1:\n",
    "                            if chunk.dst in chunks:\n",
    "                                chunks[chunk.dst].srcs.append(idx_key)\n",
    "                            else:\n",
    "                                print(f\"Warning: 存在しないチャンク番号 {chunk.dst} への依存\")\n",
    "                    sentences.append(chunks)\n",
    "                    chunks = {}\n",
    "            else:\n",
    "                if '\\t' in line:\n",
    "                    surface, feature = line.split('\\t')\n",
    "                    features = feature.split(',')\n",
    "                    if len(features) >= 7:\n",
    "                        morph = Morph(surface, features[6], features[0], features[1])\n",
    "                        chunks[idx].morphs.append(morph)\n",
    "    return sentences\n",
    "\n",
    "def extract_sahen_case(sentences):\n",
    "    results = []\n",
    "    for chunks in sentences:\n",
    "        for idx in sorted(chunks.keys()):\n",
    "            chunk = chunks[idx]\n",
    "            if not chunk.has_sahen_wo():\n",
    "                continue\n",
    "\n",
    "            sahen_wo = chunk.sahen_wo_surface()\n",
    "            if not sahen_wo:\n",
    "                continue\n",
    "\n",
    "            dst = chunk.dst\n",
    "            if dst == -1 or chunks[dst] is None:\n",
    "                continue\n",
    "\n",
    "            predicate_chunk = chunks[dst]\n",
    "            predicate = None\n",
    "            for morph in predicate_chunk.morphs:\n",
    "                if morph.pos == '動詞':\n",
    "                    predicate = morph.base\n",
    "                    break\n",
    "\n",
    "            if not predicate:\n",
    "                continue\n",
    "\n",
    "            full_predicate = sahen_wo + predicate\n",
    "\n",
    "            cases = []\n",
    "            phrases = []\n",
    "            for src in predicate_chunk.srcs:\n",
    "                src_chunk = chunks[src]\n",
    "                for morph in src_chunk.morphs:\n",
    "                    if morph.pos == '助詞':\n",
    "                        cases.append(morph.surface)\n",
    "                        phrases.append(src_chunk.surface())\n",
    "                        break\n",
    "\n",
    "            if cases:\n",
    "                cases, phrases = zip(*sorted(zip(cases, phrases)))  # 助詞でソート\n",
    "                results.append(f\"{full_predicate}\\t{' '.join(cases)}\\t{' '.join(phrases)}\")\n",
    "    return results\n",
    "\n",
    "# 入力ファイルと出力ファイルを指定\n",
    "input_filename = './datafiles/ai.ja.txt.parsed'  # 実際のパスに変更してください\n",
    "output_filename = './datafiles/sahen_cases.txt'  # 出力ファイル名\n",
    "\n",
    "# コーパスを解析\n",
    "sentences = parse_cabocha(input_filename)\n",
    "results = extract_sahen_case(sentences)\n",
    "\n",
    "# 出力ファイルに書き込む\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_f:\n",
    "    for line in results:\n",
    "        out_f.write(line + '\\n')\n",
    "\n",
    "print(f\"出力を {output_filename} に保存しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4911a6-3cb8-4e07-8522-28632e1e1207",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6065b1c7-c4f3-4144-b961-72d3a96568f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "処理が完了しました。結果は ./datafiles/output_paths.txt に出力されました。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_cabocha(file_path):\n",
    "    \"\"\"\n",
    "    CaboCha形式の解析結果を逐行処理して文節ごとの情報を生成。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = []\n",
    "        chunk = {\"morphs\": [], \"dst\": -1, \"srcs\": []}\n",
    "        for line in f:\n",
    "            if line.startswith('*'):\n",
    "                if chunk[\"morphs\"]:\n",
    "                    chunks.append(chunk)\n",
    "                    chunk = {\"morphs\": [], \"dst\": -1, \"srcs\": []}\n",
    "                dst = int(re.search(r'(\\d+)D', line).group(1))\n",
    "                chunk[\"dst\"] = dst\n",
    "            elif line.strip() == 'EOS':\n",
    "                if chunk[\"morphs\"]:\n",
    "                    chunks.append(chunk)\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    if ch[\"dst\"] != -1:\n",
    "                        chunks[ch[\"dst\"]][\"srcs\"].append(i)\n",
    "                yield chunks\n",
    "                chunks = []\n",
    "            else:\n",
    "                surface, features = line.split('\\t')\n",
    "                features = features.split(',')\n",
    "                chunk[\"morphs\"].append({\"surface\": surface, \"pos\": features[0]})\n",
    "\n",
    "def extract_paths(chunks, output_file):\n",
    "    \"\"\"\n",
    "    名詞を含む文節から構文木の根に至るパスを抽出してファイルに書き出す。\n",
    "    \"\"\"\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if any(m[\"pos\"] == \"名詞\" for m in chunk[\"morphs\"]):  # 名詞を含む文節か確認\n",
    "                path = []\n",
    "                visited = set()  # 無限ループ防止用セット\n",
    "                current = i\n",
    "                while current != -1:\n",
    "                    if current in visited:\n",
    "                        break\n",
    "                    visited.add(current)\n",
    "                    surface = \"\".join(m[\"surface\"] for m in chunks[current][\"morphs\"])\n",
    "                    path.append(surface)\n",
    "                    current = chunks[current][\"dst\"]\n",
    "                f.write(\" -> \".join(path) + '\\n')\n",
    "\n",
    "# 実行\n",
    "file_path = './datafiles/ai.ja.txt.parsed'\n",
    "output_file = './datafiles/output_paths.txt'\n",
    "\n",
    "# 出力ファイルを初期化\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "try:\n",
    "    for chunks in parse_cabocha(file_path):\n",
    "        extract_paths(chunks, output_file)\n",
    "    print(f\"処理が完了しました。結果は {output_file} に出力されました。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ファイルが見つかりません: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579997c0-7c7e-4e4f-90b6-836e482d83fb",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj\n",
    "（i<j\n",
    "）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "文節i\n",
    "とj\n",
    "に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "文節i\n",
    "から構文木の根に至る経路上に文節j\n",
    "が存在する場合: 文節i\n",
    "から文節j\n",
    "のパスを表示\n",
    "上記以外で，文節i\n",
    "と文節j\n",
    "から構文木の根に至る経路上で共通の文節k\n",
    "で交わる場合: 文節i\n",
    "から文節k\n",
    "に至る直前のパスと文節j\n",
    "から文節k\n",
    "に至る直前までのパス，文節k\n",
    "の内容を” | “で連結して表示\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597132f-1045-4e02-afc9-335e3e8299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_cabocha(file_path):\n",
    "    \"\"\"\n",
    "    CaboCha形式の解析結果を逐行処理して文節ごとの情報を生成。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = []\n",
    "        chunk = {\"morphs\": [], \"dst\": -1, \"srcs\": []}\n",
    "        for line in f:\n",
    "            if line.startswith('*'):\n",
    "                if chunk[\"morphs\"]:\n",
    "                    chunks.append(chunk)\n",
    "                    chunk = {\"morphs\": [], \"dst\": -1, \"srcs\": []}\n",
    "                dst = int(re.search(r'(\\d+)D', line).group(1))\n",
    "                chunk[\"dst\"] = dst\n",
    "            elif line.strip() == 'EOS':\n",
    "                if chunk[\"morphs\"]:\n",
    "                    chunks.append(chunk)\n",
    "                for i, ch in enumerate(chunks):\n",
    "                    if ch[\"dst\"] != -1:\n",
    "                        chunks[ch[\"dst\"]][\"srcs\"].append(i)\n",
    "                yield chunks\n",
    "                chunks = []\n",
    "            else:\n",
    "                surface, features = line.split('\\t')\n",
    "                features = features.split(',')\n",
    "                chunk[\"morphs\"].append({\"surface\": surface, \"pos\": features[0]})\n",
    "\n",
    "def replace_noun(chunk, replacement):\n",
    "    \"\"\"\n",
    "    文節内の最初の名詞を指定された文字列に置換。\n",
    "    \"\"\"\n",
    "    replaced = False\n",
    "    result = []\n",
    "    for morph in chunk[\"morphs\"]:\n",
    "        if morph[\"pos\"] == \"名詞\" and not replaced:\n",
    "            result.append(replacement)\n",
    "            replaced = True\n",
    "        else:\n",
    "            result.append(morph[\"surface\"])\n",
    "    return \"\".join(result)\n",
    "\n",
    "def find_path(chunks, start):\n",
    "    \"\"\"\n",
    "    文節 start から根に至るパスを取得。\n",
    "    \"\"\"\n",
    "    path = []\n",
    "    current = start\n",
    "    while current != -1:\n",
    "        path.append(current)\n",
    "        current = chunks[current][\"dst\"]\n",
    "    return path\n",
    "\n",
    "def write_paths_to_file(chunks, file_obj):\n",
    "    \"\"\"\n",
    "    文中の名詞句ペアを結ぶ最短係り受けパスを計算して即時ファイルに出力。\n",
    "    \"\"\"\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            if any(m[\"pos\"] == \"名詞\" for m in chunks[i][\"morphs\"]) and \\\n",
    "               any(m[\"pos\"] == \"名詞\" for m in chunks[j][\"morphs\"]):\n",
    "\n",
    "                path_i = find_path(chunks, i)\n",
    "                path_j = find_path(chunks, j)\n",
    "\n",
    "                # 文節 i から j に直接つながる場合\n",
    "                if j in path_i:\n",
    "                    path = path_i[:path_i.index(j) + 1]\n",
    "                    path_str = \" -> \".join(\n",
    "                        replace_noun(chunks[k], \"X\" if k == i else \"Y\" if k == j else \"\".join(m[\"surface\"] for m in chunks[k][\"morphs\"]))\n",
    "                        for k in path\n",
    "                    )\n",
    "                    file_obj.write(path_str + '\\n')\n",
    "                else:\n",
    "                    # 文節 i と j の経路が交差する場合\n",
    "                    common = set(path_i) & set(path_j)\n",
    "                    if common:\n",
    "                        common_node = min(common, key=path_i.index)\n",
    "                        path_i_to_common = path_i[:path_i.index(common_node)]\n",
    "                        path_j_to_common = path_j[:path_j.index(common_node)]\n",
    "\n",
    "                        path_i_str = \" -> \".join(\n",
    "                            replace_noun(chunks[k], \"X\" if k == i else \"\".join(m[\"surface\"] for m in chunks[k][\"morphs\"]))\n",
    "                            for k in path_i_to_common\n",
    "                        )\n",
    "                        path_j_str = \" -> \".join(\n",
    "                            replace_noun(chunks[k], \"Y\" if k == j else \"\".join(m[\"surface\"] for m in chunks[k][\"morphs\"]))\n",
    "                            for k in path_j_to_common\n",
    "                        )\n",
    "                        common_str = \"\".join(m[\"surface\"] for m in chunks[common_node][\"morphs\"])\n",
    "                        file_obj.write(f\"{path_i_str} | {path_j_str} | {common_str}\\n\")\n",
    "\n",
    "# 実行\n",
    "file_path = './datafiles/ai.ja.txt.parsed'\n",
    "output_file = './datafiles/output_noun_paths.txt'\n",
    "\n",
    "# 出力ファイルを初期化\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "try:\n",
    "    with open(output_file, 'a', encoding='utf-8') as out_f:\n",
    "        for chunks in parse_cabocha(file_path):\n",
    "            write_paths_to_file(chunks, out_f)\n",
    "    print(f\"処理が完了しました。結果は {output_file} に出力されました。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ファイルが見つかりません: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"エラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edafb7c-8dba-424d-97d1-d2d5b5d1010a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
