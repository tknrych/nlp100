{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06959451-0543-49d3-af95-40c44c8faf35",
   "metadata": {},
   "source": [
    "# https://nlp100.github.io/ja/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80ec45-a2e7-40e1-ab8a-264670e26554",
   "metadata": {},
   "source": [
    "## 第6章: 機械学習\n",
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6994a696-f076-4a79-85e5-6d15354f3d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/ryu/.venv/lib/python3.12/site-packages/cabocha_python-0.69-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ryu/.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809aca3-3716-4c3b-945e-0dbadbfa3cda",
   "metadata": {},
   "source": [
    "### 50. データの入手・整形\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7779d26-114a-4207-9335-a07cd257024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY: Dataset of references (urls) to news web pages\n",
      "\n",
      "DESCRIPTION: Dataset of references to news web pages collected from an online aggregator in the period from March 10 to August 10 of 2014. The resources are grouped into clusters that represent pages discussing the same news story. The dataset includes also references to web pages that point (has a link to) one of the news page in the collection.\n",
      "\n",
      "TAGS: web pages, news, aggregator, classification, clustering\n",
      "\n",
      "LICENSE: Public domain - Due to restrictions on content and use of the news sources, the corpus is limited to web references (urls) to web pages and does not include any text content. The references have been retrieved from the news aggregator through traditional web browsers. \n",
      "\n",
      "FILE ENCODING: UTF-8\n",
      "\n",
      "FORMAT: Tab delimited CSV files. \n",
      "\n",
      "DATA SHAPE AND STATS: 422937 news pages and divided up into:\n",
      "\n",
      "152746 \tnews of business category\n",
      "108465 \tnews of science and technology category\n",
      "115920 \tnews of business category\n",
      " 45615 \tnews of health category\n",
      "\n",
      "2076 clusters of similar news for entertainment category\n",
      "1789 clusters of similar news for science and technology category\n",
      "2019 clusters of similar news for business category\n",
      "1347 clusters of similar news for health category\n",
      "\n",
      "References to web pages containing a link to one news included in the collection are also included. They are represented as pairs of urls corresponding to 2-page browsing sessions. The collection includes 15516 2-page browsing sessions covering 946 distinct clusters divided up into:\n",
      "\n",
      "6091 2-page sessions for business category\n",
      "9425 2-page sessions for entertainment category\n",
      "\n",
      " \n",
      "\n",
      "CONTENT\n",
      "=======\n",
      "\n",
      "FILENAME #1: newsCorpora.csv (102.297.000 bytes)\n",
      "DESCRIPTION: News pages\n",
      "FORMAT: ID \\t TITLE \\t URL \\t PUBLISHER \\t CATEGORY \\t STORY \\t HOSTNAME \\t TIMESTAMP\n",
      "\n",
      "where:\n",
      "ID\t\tNumeric ID\n",
      "TITLE\t\tNews title \n",
      "URL\t\tUrl\n",
      "PUBLISHER\tPublisher name\n",
      "CATEGORY\tNews category (b = business, t = science and technology, e = entertainment, m = health)\n",
      "STORY\t\tAlphanumeric ID of the cluster that includes news about the same story\n",
      "HOSTNAME\tUrl hostname\n",
      "TIMESTAMP \tApproximate time the news was published, as the number of milliseconds since the epoch 00:00:00 GMT, January 1, 1970\n",
      "\n",
      "\n",
      "FILENAME #2: 2pageSessions.csv (3.049.986 bytes)\n",
      "DESCRIPTION: 2-page sessions\n",
      "FORMAT: STORY \\t HOSTNAME \\t CATEGORY \\t URL\n",
      "\n",
      "where:\n",
      "STORY\t\tAlphanumeric ID of the cluster that includes news about the same story\n",
      "HOSTNAME\tUrl hostname\n",
      "CATEGORY\tNews category (b = business, t = science and technology, e = entertainment, m = health)\n",
      "URL\t\tTwo space-delimited urls representing a browsing session\n"
     ]
    }
   ],
   "source": [
    "!cat ./datafiles/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ef169c-e19c-479a-a733-714345d26b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1  Fed official says weak data caused by weather,...   \n",
       "1   2  Fed's Charles Plosser sees high bar for change...   \n",
       "2   3  US open: Stocks fall after Fed official hints ...   \n",
       "3   4  Fed risks falling 'behind the curve', Charles ...   \n",
       "4   5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "2  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "3  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "4  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df = pd.read_csv('./datafiles/newsCorpora.csv', sep='\\t', header=None, names=column_names)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ced8f5-681b-413b-a0f1-4e4209d79826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Europe reaches crunch point on banking union</td>\n",
       "      <td>http://in.reuters.com/article/2014/03/10/eu-ba...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>b</td>\n",
       "      <td>dPhGU51DcrolUIMxbRm0InaHGA2XM</td>\n",
       "      <td>in.reuters.com</td>\n",
       "      <td>1394470501755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>ECB FOCUS-Stronger euro drowns out ECB's messa...</td>\n",
       "      <td>http://in.reuters.com/article/2014/03/10/ecb-p...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>b</td>\n",
       "      <td>dPhGU51DcrolUIMxbRm0InaHGA2XM</td>\n",
       "      <td>in.reuters.com</td>\n",
       "      <td>1394470501948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Euro Anxieties Wane as Bunds Top Treasuries, S...</td>\n",
       "      <td>http://www.businessweek.com/news/2014-03-10/ge...</td>\n",
       "      <td>Businessweek</td>\n",
       "      <td>b</td>\n",
       "      <td>dPhGU51DcrolUIMxbRm0InaHGA2XM</td>\n",
       "      <td>www.businessweek.com</td>\n",
       "      <td>1394470503148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Noyer Says Strong Euro Creates Unwarranted Eco...</td>\n",
       "      <td>http://www.businessweek.com/news/2014-03-10/no...</td>\n",
       "      <td>Businessweek</td>\n",
       "      <td>b</td>\n",
       "      <td>dPhGU51DcrolUIMxbRm0InaHGA2XM</td>\n",
       "      <td>www.businessweek.com</td>\n",
       "      <td>1394470503366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>REFILE-Bad loan triggers key feature in ECB ba...</td>\n",
       "      <td>http://in.reuters.com/article/2014/03/10/euroz...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>b</td>\n",
       "      <td>dPhGU51DcrolUIMxbRm0InaHGA2XM</td>\n",
       "      <td>in.reuters.com</td>\n",
       "      <td>1394470505070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              TITLE  \\\n",
       "12  13       Europe reaches crunch point on banking union   \n",
       "13  14  ECB FOCUS-Stronger euro drowns out ECB's messa...   \n",
       "19  20  Euro Anxieties Wane as Bunds Top Treasuries, S...   \n",
       "20  21  Noyer Says Strong Euro Creates Unwarranted Eco...   \n",
       "29  30  REFILE-Bad loan triggers key feature in ECB ba...   \n",
       "\n",
       "                                                  URL     PUBLISHER CATEGORY  \\\n",
       "12  http://in.reuters.com/article/2014/03/10/eu-ba...       Reuters        b   \n",
       "13  http://in.reuters.com/article/2014/03/10/ecb-p...       Reuters        b   \n",
       "19  http://www.businessweek.com/news/2014-03-10/ge...  Businessweek        b   \n",
       "20  http://www.businessweek.com/news/2014-03-10/no...  Businessweek        b   \n",
       "29  http://in.reuters.com/article/2014/03/10/euroz...       Reuters        b   \n",
       "\n",
       "                            STORY              HOSTNAME      TIMESTAMP  \n",
       "12  dPhGU51DcrolUIMxbRm0InaHGA2XM        in.reuters.com  1394470501755  \n",
       "13  dPhGU51DcrolUIMxbRm0InaHGA2XM        in.reuters.com  1394470501948  \n",
       "19  dPhGU51DcrolUIMxbRm0InaHGA2XM  www.businessweek.com  1394470503148  \n",
       "20  dPhGU51DcrolUIMxbRm0InaHGA2XM  www.businessweek.com  1394470503366  \n",
       "29  dPhGU51DcrolUIMxbRm0InaHGA2XM        in.reuters.com  1394470505070  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fabac0-0cbe-4471-a353-3c9afbd2b8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PUBLISHER\n",
       "Reuters             3902\n",
       "Huffington Post     2455\n",
       "Businessweek        2395\n",
       "Contactmusic.com    2334\n",
       "Daily Mail          2254\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PUBLISHER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9216c2c2-d64c-4d87-ae4f-3d17557c0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "import numpy as np\n",
    "shuffled_indices = np.random.permutation(df.index)\n",
    "df = df.loc[shuffled_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be1212e0-29f3-49e4-b4b8-1e4801c78701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>Justin Bieber Avoids Felony Charge In Alleged ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>Lorillard Reaches Record on Fresh Reynolds Tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>UPDATE 1-Juniper's revenue rises as telecom cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>Emma Stone and Colin Firth in Magic In The Moo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>UK inflation hits new four-year low in Februar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>UPDATE 1-California's proposed 2015 Obamacare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>UPDATE 2-China PMIs fuel hope economy is stabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e</td>\n",
       "      <td>Bedridden Miley Cyrus loses her 'brain' in The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e</td>\n",
       "      <td>â€˜Tammyâ€™ Proves No Match For â€˜Transformer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b</td>\n",
       "      <td>Twitter Insiders Plan to Hold Stock Even as Lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CATEGORY                                              TITLE\n",
       "0        e  Justin Bieber Avoids Felony Charge In Alleged ...\n",
       "1        b  Lorillard Reaches Record on Fresh Reynolds Tak...\n",
       "2        b  UPDATE 1-Juniper's revenue rises as telecom cl...\n",
       "3        e  Emma Stone and Colin Firth in Magic In The Moo...\n",
       "4        b  UK inflation hits new four-year low in Februar...\n",
       "5        b  UPDATE 1-California's proposed 2015 Obamacare ...\n",
       "6        b  UPDATE 2-China PMIs fuel hope economy is stabi...\n",
       "7        e  Bedridden Miley Cyrus loses her 'brain' in The...\n",
       "8        e  â€˜Tammyâ€™ Proves No Match For â€˜Transformer...\n",
       "9        b  Twitter Insiders Plan to Hold Stock Even as Lo..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['CATEGORY', 'TITLE']]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "963c600d-43a7-4815-95fe-f4836a6fa416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 10672\n",
      "Validation data size: 1334\n",
      "Test data size: 1334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# まず、データを80%と20%に分割\n",
    "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 残りの20%を検証データと評価データに分割\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78dc353e-2579-4c1b-9220-f811aceb03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# タブ区切りで各データセットをファイルに保存\n",
    "train_data.to_csv(\"./datafiles/train.txt\", sep=\"\\t\", index=False)\n",
    "validation_data.to_csv(\"./datafiles/valid.txt\", sep=\"\\t\", index=False)\n",
    "test_data.to_csv(\"./datafiles/test.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfa790d-27ba-4b30-aca2-20684fbc4c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 10672\n",
      "Validation data size: 1334\n",
      "Test data size: 1334\n"
     ]
    }
   ],
   "source": [
    "# 各データセットのサイズを確認\n",
    "print(\"Train data size:\", len(train_data))\n",
    "print(\"Validation data size:\", len(validation_data))\n",
    "print(\"Test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187890cd-6afe-4b6b-b851-db444fae0ed4",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94e6f06a-b834-4e19-b5a2-4b58834b464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベースライン：見出しを単語列に変換\n",
    "def extract_features(data):\n",
    "    # 単語列を作成（例としてスペース区切りで分割）\n",
    "    data['features'] = data['TITLE'].str.split()\n",
    "    return data[['features', 'CATEGORY', 'TITLE']]\n",
    "\n",
    "# 学習データ、検証データ、評価データから特徴量を抽出\n",
    "train_features = extract_features(train_data)\n",
    "validation_features = extract_features(validation_data)\n",
    "test_features = extract_features(test_data)\n",
    "\n",
    "# 各データをファイルに保存\n",
    "train_features.to_csv(\"./datafiles/train.feature.txt\", sep=\"\\t\", index=False, header=False)\n",
    "validation_features.to_csv(\"./datafiles/valid.feature.txt\", sep=\"\\t\", index=False, header=False)\n",
    "test_features.to_csv(\"./datafiles/test.feature.txt\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678c8c1-a90f-48a5-bbbf-cc8ce1dc96e7",
   "metadata": {},
   "source": [
    "## 52. 学習\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1266acb8-7a76-4c46-94ff-0d8a560fc30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           e       0.90      0.98      0.93       558\n",
      "           b       0.89      0.94      0.91       520\n",
      "           m       0.97      0.61      0.75       109\n",
      "           t       0.86      0.59      0.70       147\n",
      "\n",
      "    accuracy                           0.89      1334\n",
      "   macro avg       0.90      0.78      0.83      1334\n",
      "weighted avg       0.89      0.89      0.89      1334\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           e       0.89      0.98      0.93       524\n",
      "           b       0.88      0.95      0.91       556\n",
      "           m       0.94      0.46      0.62       100\n",
      "           t       0.87      0.60      0.71       154\n",
      "\n",
      "    accuracy                           0.88      1334\n",
      "   macro avg       0.89      0.75      0.79      1334\n",
      "weighted avg       0.88      0.88      0.87      1334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# カテゴリを数値に変換\n",
    "category_to_index = {category: idx for idx, category in enumerate(df[\"CATEGORY\"].unique())}\n",
    "index_to_category = {v: k for k, v in category_to_index.items()}  # インデックスからカテゴリ名のマッピング\n",
    "train_data[\"CATEGORY\"] = train_data[\"CATEGORY\"].map(category_to_index)\n",
    "validation_data[\"CATEGORY\"] = validation_data[\"CATEGORY\"].map(category_to_index)\n",
    "test_data[\"CATEGORY\"] = test_data[\"CATEGORY\"].map(category_to_index)\n",
    "\n",
    "# TF-IDFベクトル化\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data[\"TITLE\"])\n",
    "X_validation = vectorizer.transform(validation_data[\"TITLE\"])\n",
    "X_test = vectorizer.transform(test_data[\"TITLE\"])\n",
    "\n",
    "y_train = train_data[\"CATEGORY\"]\n",
    "y_validation = validation_data[\"CATEGORY\"]\n",
    "y_test = test_data[\"CATEGORY\"]\n",
    "\n",
    "# ロジスティック回帰モデルの学習\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# モデル評価\n",
    "validation_pred = model.predict(X_validation)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# カテゴリ名リストを作成\n",
    "target_names = [index_to_category[i] for i in sorted(index_to_category.keys())]\n",
    "\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_validation, validation_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff12f3-7d38-4b0b-86a6-a55037d004d1",
   "metadata": {},
   "source": [
    "## 53. 予測\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63ee69-fb2c-4a0e-9823-3a45ebb5a683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
